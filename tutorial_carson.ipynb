{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "040419e6",
   "metadata": {},
   "source": [
    "# A RETRO tutorial by Carson Lam \n",
    "\n",
    "this is my notebook to learn and teach the implementation of <a href=\"https://arxiv.org/abs/2112.04426\">RETRO</a>, Deepmind's Retrieval based Attention net, in Pytorch, on a small but meaningful task. \n",
    "\n",
    "1. I am using python3.8, if you dont have python 3.8 there are many ways to install it, using pyenv or homebrew, I used homebrew  and followed the instructions at the end of the download. You have to restart your terminal for the changes to take effect.\n",
    "\n",
    "2. create a virtual environment for this project and entered that environment\n",
    "\n",
    "```\n",
    "python3.8 -m venv env\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "3. install this project's dependencies from requirements.txt\n",
    "\n",
    "```\n",
    "pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "4. save any additional dependencies you have pip installed inside your environment along with the specific version back into requirements.txt for later use\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "\n",
    "5. open up jupyter and open this notebook\n",
    "\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "6. outside of the RETRO-pytorch folder we have a folder called data/ and inside data/ we have text_folder/ and processed_text/ folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c4c836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version 1.11.0\n",
      "torch.cuda.is_available() False\n",
      "torch.cuda.device_count() 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from retro_pytorch import RETRO, TrainingWrapper\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print('torch.version', torch.__version__)\n",
    "print('torch.cuda.is_available()', torch.cuda.is_available())\n",
    "print('torch.cuda.device_count()', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b28294b",
   "metadata": {},
   "source": [
    "the chunk size that is indexed and retrieved is needed for proper relative positions as well as causal chunked cross attention\n",
    "\n",
    "decoder cross attention layers is used with causal chunk cross attention\n",
    " \n",
    "turn on `use_deepnet`  post-normalization with DeepNet residual scaling and initialization,  for scaling to 1000 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f11e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "retro = RETRO(\n",
    "    chunk_size = 64,                         # the chunk size that is indexed and retrieved  \n",
    "    max_seq_len = 2048,                      # max sequence length\n",
    "    enc_dim = 2, #896                        # encoder model dim\n",
    "    enc_depth = 2,                           # encoder depth\n",
    "    dec_dim = 2, #768,                       # decoder model dim\n",
    "    dec_depth = 6, #12                       # decoder depth\n",
    "    dec_cross_attn_layers = (3, 6), #(3, 6, 9, 12),   # decoder cross attention layers \n",
    "    heads = 1, #8                            # attention heads\n",
    "    dim_head = 32, #64                       # dimension per head\n",
    "    dec_attn_dropout = 0.25,                 # decoder attention dropout\n",
    "    dec_ff_dropout = 0.25,                   # decoder feedforward dropout\n",
    "    use_deepnet = True                       # turn on post-normalization with DeepNet residual scaling and initialization \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a3f774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5077, 13497, 19083,  ...,   602, 17115, 17246],\n",
      "        [12548,  4856,  6550,  ..., 11688, 17976,  6109]])\n",
      "torch.Size([2, 2049])\n"
     ]
    }
   ],
   "source": [
    " # plus one since it is split into input and labels for training\n",
    "seq = torch.randint(0, 20000, (2, 2048 + 1))   \n",
    "print(seq)\n",
    "print(seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a37b6ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1601, 19473,  9206, 14366, 14221,  7337,  9191, 13800, 13641, 14119,\n",
      "           10225,    57,  1722,  3420,  9363,  6131, 15305, 10109,  6006,  4499,\n",
      "            3986,  4643,  8430,  7113,   159, 14452, 17571,  6632,  7755,  6512,\n",
      "           18741,  8702]],\n",
      "\n",
      "         [[17144, 19027, 14726, 14819,  9710,  1291,  3069, 12937, 18709,  8912,\n",
      "            7244, 17348,  9276, 17261,  7435, 12393, 18171,  3626,  6282,  4880,\n",
      "           17808, 18561,  9615, 13768,  2940,  5774, 16365, 17641,  8120, 14100,\n",
      "           13870,  6459]],\n",
      "\n",
      "         [[18666,   579,   561, 18489,  7426, 18334,  1080,  2945,  6500,   632,\n",
      "            1838,  2085,  8767, 14453, 12808,  1028,  4739,  7120, 11321, 12055,\n",
      "           14965,  7599, 12952, 17885, 19124,  6459,   612,  9428, 10531, 16981,\n",
      "            2916,  3092]]]])\n",
      "torch.Size([2, 32, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "# retrieved tokens \n",
    "# - (batch, num chunks, num retrieved neighbors, retrieved chunk with continuation)\n",
    "retrieved = torch.randint(0, 20000, (2, 32, 2, 128)) \n",
    "print(retrieved[:1,:3,:1,:32])\n",
    "print(retrieved.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f0f730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.5088, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = retro(seq, retrieved, return_loss = True)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c06ebc",
   "metadata": {},
   "source": [
    "The aim of the TrainingWrapper is to process a folder of text documents into the necessary memmapped numpy arrays to begin training RETRO.\n",
    "\n",
    "`bert_embed()` will automatically use cuda if available so best to match it with the retro that is inputted to wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/text_folder/doc1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/carson/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/text_folder/doc2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/carson/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    retro = retro.cuda()\n",
    "\n",
    "wrapper = TrainingWrapper(\n",
    "    retro = retro,                                 # path to retro instance\n",
    "    knn = 2,                                       # knn (2 in paper was sufficient)\n",
    "    chunk_size = 32,                               # chunk size (64 in paper)\n",
    "    documents_path = '../data/text_folder',              # path to folder of text\n",
    "    glob = '**/*.txt',                             # text glob\n",
    "    chunks_memmap_path = '../data/processed_text/train.chunks.dat',     # path to chunks\n",
    "    seqs_memmap_path = '../data/processed_text/train.seq.dat',          # path to sequence data\n",
    "    doc_ids_memmap_path = '../data/processed_text/train.doc_ids.dat',   # path to document ids per chunk (used for filtering neighbors belonging to same document)\n",
    "    max_chunks = 10,                         # maximum cap to chunks\n",
    "    max_seqs = 10,                            # maximum seqs\n",
    "    knn_extra_neighbors = 2,                     # num extra neighbors to fetch\n",
    "    max_index_memory_usage = '10m',\n",
    "    current_memory_available = '0.5G'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6764c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
